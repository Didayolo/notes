\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{here}
\usepackage{amsmath}
\usepackage{amsfonts}

\title{Notes de cours\\OPT6 : Apprentissage avancé}
\author{Adrien Pavao}
\date{Novembre 2017}

\begin{document}

\maketitle

%\tableofcontents

\section{Introduction}

L'apprentissage automatique (machine learning) s'appuie sur des bases théoriques. Ce domaine évolue vite et on y trouve des théories et techniques avancées.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.4]{outline.png}
  \caption{Les points qui seront abordés dans ce cours.}
\end{figure}

\subsection{Quelques notions}
\begin{itemize}
\item Vladimir Vapnik a posé les bases théoriques de l'apprentissage statistique\footnote{https://en.wikipedia.org/wiki/Statistical\_learning\_theory}.
\item Apprentissage PAC : Probably Approximately Correct\footnote{https://en.wikipedia.org/wiki/Probably\_approximately\_correct\_learning}.
\item \textbf{No-free-lunch theorem :} Il n'existe pas de méthode d'apprentissage qui soit meilleure qui les autres. Même performance que l'aléatoire sur l'ensemble des problèmes possibles. Si une méthode est efficace sur un problème elle sera mauvaise sur un autre. L'intégrale sur tous les problèmes est toujours la même, quelque soit la méthode. Idem pour les mesures (de distance par exemple). Les méthodes sont adaptées à des \textbf{classes} de problèmes. \textbf{Tous les algorithmes inductifs se valent.}
\item \textbf{Adversarial learning :} L'intersection entre le machine learning et la sécurité informatique.
\item \textbf{Régularisation :} Un processus consistant à ajouter de l'information à un problème pour éviter le surapprentissage. Cette information prend généralement la forme d'une pénalité envers la complexité du modèle. On peut relier cette méthode au principe du \textbf{rasoir d'Occam}. D'un point de vue bayésien, l'utilisation de la régularisation revient à imposer une distribution a priori sur les paramètres du modèle.

Une méthode généralement utilisée est de pénaliser les valeurs extrêmes des paramètres, qui correspondent souvent à un surapprentissage. Pour cela, on va utiliser une norme sur ces paramètres, que l'on va ajouter à la fonction qu'on cherche à minimiser. Les normes les plus couramment employées pour cela sont $L_1$ et $L_2$.

\item Différentes normes permettent de calculer la taille totale d'un vecteur ou d'une matrice dans un espace vectoriel. Quelques exemples courants : 

      Norme $L_0$ :
            \[ ||x||_0 = \sqrt[0]{\sum_i x_i^0} \]
      Norme $L_1$ :
            \[ ||x||_1 = \sum_i |x_i| \]
      Norme $L_2$ :
            \[ ||x||_2 = \sqrt{\sum_i x^2_i} \]
            
\item \textbf{Distance de Manhattan :} La distance entre deux points lorsque les déplacements sont faits selon un réseau ou un quadrillage. 

\begin{figure}[H]
    \begin{center} \includegraphics[scale=0.5]{manhattan.png} \end{center}
     \caption{Distance de Manhattan (chemins rouge, jaune et bleu) contre distance euclidienne en vert.}
\end{figure}

Le nom de cette distance fait sans doute référence à l'architecture en quadrillage du quartier de Manhattan, à New York.

\item Référence d'un livre intéressant : \textit{Hofstadter - "Godel, Escher, Bach".}
\item \textbf{Complexité de Kolmogorov :} Une fonction permettant de quantifier la taille du plus petit algorithme nécessaire pour engendrer un nombre ou une suite quelconque de caractères. Cette quantité peut être vue comme une évaluation d'une forme de complexité de cette suite de caractères.
\item Effets de séquences : Une séquence d'information influe sur le résultat de l'induction (à préciser).
\item L'algorithme d'apprentissage du perceptron converge si les données sont linéairement séparables. Cette convergence est en nombre fini d'étapes et ce nombre est indépendant du nombres d'exemples, de la distribution des exemples, et quasiment pas de la dimension de l'espace d'entrée. Il dépend de la marge de séparation des nuages de points et du diamètre de la boule (à préciser).
\item \textbf{L'espace des versions :} Toutes les hypothèses correctes (qui ne font pas d'erreurs) d'après les données d'apprentissage.
\end{itemize}

\section{Induction}

On déduit à partir d'un échantillon.

\begin{enumerate}
\item Espace d'hypothèse H.
\item Critere inductif $ S \times h \in H \rightarrow $ Score $\in \mathbb{R}$. Par exemple minimiser le taux d'erreur.
\item Methode d'exploration de H.
\end{enumerate}

On ne dispose pas de théorie bien établie de l'induction (exemple Fibonacci).

\subsection{Différents types d'apprentissage}
\begin{itemize}
\item \textbf{Descriptif :} Non supervisé. On cherche des régularités dans les données. On ne souhaite pas extrapoler, on ne s'interesse qu'à l'échantillon de données dont on dispose. La "matière noire" de l'apprentissage.
\item \textbf{Prédictif :} Supervisé. L'échantillon de données sert à apprendre une hypothèse sur les données pour prédire ensuite sur de nouvelles données. On cherche des corrélations entre les données.
\item \textbf{Prescriptif :} On cherche des \textbf{causalités}. Il s'agit d'une tâche difficile.
\end{itemize}

Les frontières entre ces types d'apprentissage peuvent être floues, on peut par exemple commencer par une description des données pour les comprendre avant de faire un modèle prédictif.

\subsection{Différentes méthodes d'apprentissage}

Dans ce cours nous allons aborder différentes méthodes d'apprentissage.

\begin{itemize}

\item Méthodes d'ensemble : Combiner plusieurs hypothèse (exemple : boosting).

\item Supervisé.

\item L'induction :
    \begin{itemize}
    \item Apprentissage semi supervisé 
    \item Apprentissage de modèles parcimonieux (peu de paramètres).
    \end{itemize}

\item Apprentissage en ligne (online learning) : Les données arrivent en cours de route.

\item \textbf{Apprentissage par transfert :} Cela vise à transférer des connaissances d'une ou plusieurs tâches sources vers une ou plusieurs tâches cibles. On peut le voir comme la capacité d’un système à reconnaître et appliquer des connaissances et des compétences, apprises à partir de tâches antérieures, sur de nouvelles tâches ou domaines partageant des similitudes.

\end{itemize}

\subsection{Types de biais}

L'induction necessite d'être biaisé, d'avoir une preference pour certaines hypothèses, de ne pas connaitre toutes les fonctions possibles. Un biais classique et très humain est la préférence des hypothèses les plus simples (rasoir d'Ockham).

\begin{itemize}
\item \textbf{Biais de représentation (déclaratif) :} On ne peut représenter qu'un petit nombre de fonctions possibles. Le langage dans lequel on exprime le problème ne me permet pas de tout représenter.
\item \textbf{Biais de recherche (procédural) :} La procédure de recherche avantage certaines hypothèses. Par exemple on reste proche de notre point initial de recherche dans l'espace des fonctions possibles.
\end{itemize}

Souvent un peu des deux.

\subsection{Risques}

On distingue deux types de risques :

\begin{itemize}
\item \textbf{Risque réel} (ce que l'on veut minimiser). La performance à venir si j'utilise l'hypothèse h. Cela prend la forme d'une espérance de coût d'usage de h. Le risque réel (loss function) : 
\[ R(h) = \displaystyle \int_X^Y l(h(x), y) p(x, y) \mathrm{d}x \mathrm{d}y \]

La meilleure hypothèse est $k^* = Argmin R(h)$ avec $h \in H$. En d'autres termes, on cherche l'hypothèse qui minimise le risque réel. $l$ est la métrique d'évaluation et $p$ la loi de probabilité hypothétique ayant générée la distribution des données.
\item \textbf{Risque empirique}. Puisque l'on cherche la loi de probabilité, on ne la connait pas et on ne peut donc pas s'en servir pour calculer le risque, en pratique. On calcul donc le risque empirique : 
\[ \hat{R}(h) = \frac{1}{m} \sum_{i=1}^m l(h(x_i), y_i) \]
C'est donc la moyenne des distances entre les prédictions avec h et les valeurs réelles des données. \^h est l'hypothèse minimisant le risque empirique. 
\end{itemize}

On cherche les liens entre toutes ces notions.

\subsubsection*{Pour Vladimir Vapnik}

Plutôt que de minimiser le risque empirique : 
\begin{itemize}
\item Pour une hypothèse h : $ P_{S \sim P_{XY}} (\hat{R}(h) = 0 \wedge R(h) > \epsilon ) $
\end{itemize}
On ne veux pas d'une hypothèse qui paraisse bonne mais qui soit en réalité mauvaise. Le principe de minimisation du risque empirique n'est sain que s'il y a des contraintes sur l'espace des hypothèses.

(formule et principe de consistance universelle ?)

\section{Transduction}

On ne s'interesse qu'à un point. Le nombre de données requises pour tirer une conclusion n'est plus influencé par le nombre de dimension. On parle de \textbf{transductive learning}.
\begin{itemize}
\item Transduction : Cas d'induction limite (une question).
\item Analogie : Cas de transduction limite (un exemple).
\end{itemize}

\section{Possibilités d'apprendre}

\subsection{Perceptron}

\begin{enumerate}
\item On suppose que l'ensemble des hypothèses H est fini.
\item \textbf{Cas réalisable :} $ \exists h \in H tq R(h) = 0 $ ($ \ne 0 \rightarrow$ cas non-réalisable.)
\item S est supposé tiré \textit{i.i.d.}
\end{enumerate}


\subsection{Dimension Vapnik-Chervonenkis}

La $d_{VC}$ est la taille du plus grand ensemble de points tirés aléatoirement que l'on peut pulvériser, c'est-à-dire que l'on peut étiqueter n'importe comment en utilisant une hypothèse de H.

$d_{VC}(H)$ donne une idée de l'expressivité du modèle (de l'espace de réalisation).

\subsection{Apprentissage faible et fort}

\begin{itemize}
\item \textbf{Apprenant faible :} Erreur réelle $< \frac{1}{2} - \gamma$, $\gamma > 0$. Typiquement un decision stump.
\item \textbf{Apprenant fort :} On peut être infiniment exigeant. $\forall \epsilon$, $\forall \delta$, avec suffisamment d'exemples, A trouvera une hypothèse h d'erreur réelle $< \epsilon$ dans plus de $1 - \delta$ des cas (échantillons). Dans le cas réalisable.
\end{itemize}

\subsection{Borne sur le risque réel}

\includegraphics[scale=0.4]{risk_bound.png}

\end{document}


